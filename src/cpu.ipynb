{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3482c8c2",
   "metadata": {},
   "source": [
    "# Empirical Analysis of Early Stopping and Learning Rate Scheduling for Deep Learning on Resource-Constrained Hardware\n",
    "\n",
    "This file serves as the codebase for doing the experimentations done in the paper entitled as ***\"Empirical Analysis of Early Stopping and Learning Rate Scheduling for Deep Learning on Resource-Constrained Hardware\"***.\n",
    "\n",
    "This specific file serves as the CPU testing notebook, separating it from the ~~[Colab](./colab.ipynb) and~~ [GPU](./gpu.ipynb) experimentation notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b584f9",
   "metadata": {},
   "source": [
    "**Purpose:**\n",
    "\n",
    "To provide an empirical analysis on how specific training optimizations perform under resource-constrained hardwares. This explores consumer-grade hardware typically accessible to students and researchers with limited resources, allowing a large part of the Deep Learning community to be able to run potentially more complex tasks on such readily available devices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817aee0c",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "All needed libraries will be imported here.\n",
    "\n",
    "Unless conditional, all imports must be done in this section to prevent workspace cluttering. Imports are sorted in an ascending manner, starting from \"a\" to \"Z\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ce006e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Disable GPU for this notebook run (since this is specifically for CPU experiment).\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "from collections import deque\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, Input\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from typing import Any, Dict, Union\n",
    "import datetime\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import psutil\n",
    "import random\n",
    "import seaborn as sns\n",
    "import subprocess as sp\n",
    "import tensorflow as tf\n",
    "import threading\n",
    "import time\n",
    "\n",
    "try:\n",
    "    import GPUtil\n",
    "    gputil_exist = True\n",
    "except ImportError:\n",
    "    gputil_exist = False\n",
    "\n",
    "if (os.name == 'nt'):\n",
    "    import wmi\n",
    "    import winsound\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba261b75",
   "metadata": {},
   "source": [
    "## Data and Variables\n",
    "\n",
    "Sets all the global data and variables here.\n",
    "\n",
    "Global variables will be defined and instantiated in this section, preventing a confusing clutter down the line and allowing readability when revisions are needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaaca442",
   "metadata": {},
   "source": [
    "### Instantiations\n",
    "\n",
    "Instantiations of objects will be done here, preventing mixture of variable preview and definition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e985d7e5",
   "metadata": {},
   "source": [
    "#### Variables\n",
    "\n",
    "Variable instantiations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e37ef9b",
   "metadata": {},
   "source": [
    "##### Dataset and Model Variables\n",
    "\n",
    "All dataset and model related variables are defined here, separated from the logger related ones, allowing cohesion in the already chaotic ~~world~~ notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731baf16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = {\n",
    "    'class_names': [\n",
    "\t\t'airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "\t\t'dog', 'frog', 'horse', 'ship', 'truck'\n",
    "    ],\n",
    "    # 40k images\n",
    "\t'train': {\n",
    "\t\t'x': None,\n",
    "\t\t'y': None,\n",
    "\t},\n",
    "    # 10k images\n",
    "\t'test': {\n",
    "\t\t'x': None,\n",
    "\t\t'y': None,\n",
    "\t},\n",
    "    # 10k images\n",
    "    'val': {\n",
    "        'x': None,\n",
    "        'y': None,\n",
    "    },\n",
    "}\n",
    "(data['train']['x'], data['train']['y']), (data['test']['x'], data['test']['y']) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor = 'val_loss',\n",
    "        patience = 10,\n",
    "        verbose = 2,\n",
    "        restore_best_weights = True\n",
    "    ),\n",
    "    # ReduceLROnPlateau(\n",
    "    #     monitor = 'val_loss',\n",
    "    #     factor = 0.5,\n",
    "    #     patience = 5,\n",
    "    #     verbose = 2,\n",
    "    #     min_lr = 1e-6\n",
    "    # )\n",
    "]\n",
    "\n",
    "save_plots = True\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da8bb6a",
   "metadata": {},
   "source": [
    "##### Metrics related variables\n",
    "\n",
    "(Static) Variables that will be used by the metric logger. Some variables are localized to the country where the researcher is based.\n",
    "\n",
    "When recreating the experiment, feel free to modify the values of the following variables to match your location's costs:\n",
    "\n",
    "- `local_currency_name`: The name of your local currency *(i.e.: Philippine Peso -> `\"peso\"`)*\n",
    "- `to_usd_conversion`: The cost of your location's currency to USD.\n",
    "- `cost_per_kwh_local`: The killowatt/hour cost in your location. Use local currency values.\n",
    "\n",
    "Some other variables are also changeable to allow the researcher(s) to easily update their values such as the `cpu_tdp`:\n",
    "\n",
    "- `platform`: The platform on where the experiment was done. In the researcher's case, it is `\"XXX\"`.\n",
    "- `cpu_tdp`: The CPU's themal design power (TDP) in watts. This is different for every CPU.\n",
    "- `room_temp`: The current room temperature of where the hardware is located while the experiment is being conducted.\n",
    "\n",
    "These last two variables are for the logging:\n",
    "\n",
    "- `prefix`: The location on where the log will be saved. Could be a relative or an absolute path.\n",
    "- `file_name`: The name of the log file. Must be a JSON format... but realistically, it could be any file extension. In the end, it'll still log it in JSON. ~~LMAO~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824a70e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The local currency used by the researcher (PHP -> peso)\n",
    "local_currency_name = \"peso\"\n",
    "\"\"\"\n",
    "Defines the name of the local currency used by the researcher. This is to simplify\n",
    "and allow flexible changes to the local currency when someone wishes to recreate the\n",
    "experiment in a different country or region.\n",
    "\"\"\"\n",
    "\n",
    "# As of May 30, 2025 (PHP -> USD)\n",
    "to_usd_conversion = 0.018\n",
    "\"\"\"\n",
    "Defines the conversion rate of the local currency to US Dollar.\n",
    "\"\"\"\n",
    "\n",
    "# kwh in local currency (in this case, PHP)\n",
    "cost_per_kwh_local = 12.26\n",
    "\"\"\"\n",
    "A fixed cost per kilowatt-hour in the local currency; the currency used by the country where\n",
    "the researcher is based.\n",
    "\"\"\"\n",
    "\n",
    "# kwh in US Dollar\n",
    "cost_per_kwh_usd = cost_per_kwh_local * to_usd_conversion\n",
    "\"\"\"\n",
    "A dollar conversion of the cost per kilowatt-hour from the local currency.\n",
    "\"\"\"\n",
    "\n",
    "platform = \"Intel i5-11400H\"\n",
    "\"\"\"\n",
    "Defines the platform used for the experiment.\n",
    "\"\"\"\n",
    "\n",
    "cpu_tdp = 45\n",
    "\"\"\"\n",
    "Defines the thermal design power (TDP) of the CPU in watts.\n",
    "\"\"\"\n",
    "\n",
    "env_temp = 32.3\n",
    "\"\"\"\n",
    "Manual temperature setting for the time the experiment was conducted. The researcher used the\n",
    "location's temperature based on the weather station at the time of the experiment. However,\n",
    "it would be better to use a temperature sensor to get the temperature of the room where the\n",
    "experiment was conducted for a more accurate result.\n",
    "\"\"\"\n",
    "\n",
    "prefix = \"./../out/cpu\"\n",
    "\"\"\"\n",
    "Defines the output directory for the training metrics.\n",
    "\"\"\"\n",
    "\n",
    "file_name = lambda postfix=None: f\"{prefix}/training_metrics{f'_{postfix}' if postfix is not None and len(postfix) > 0 else ''}.log\"\n",
    "\"\"\"\n",
    "Defines the file name for the training metrics.\n",
    "\"\"\"\n",
    "\n",
    "file_postfix = f\"{platform.replace(' ', '_')}\"\n",
    "\"\"\"\n",
    "A postfix for the output files to differentiate between different runs or experiments.\n",
    "This can be used to append a unique identifier or timestamp to the file name.\n",
    "\"\"\"\n",
    "\n",
    "run_number = 1\n",
    "\"\"\"\n",
    "Defines the current run's iteration number. This is useful for tracking multiple runs\n",
    "or experiments, allowing the researcher to differentiate between them.\n",
    "\n",
    "This can be incremented for each new run to ensure that the output files do not\n",
    "overwrite each other.\n",
    "\"\"\"\n",
    "\n",
    "for strat in callbacks:\n",
    "    file_postfix += f\"-{strat.__class__.__name__}\"\n",
    "file_postfix += f\"-run_{run_number}\"\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38e0f14",
   "metadata": {},
   "source": [
    "#### Functions\n",
    "\n",
    "Function creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c995bf8",
   "metadata": {},
   "source": [
    "##### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7ce9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def showImg(input, title = None, axis = False):\n",
    "    \"\"\"\n",
    "    Displays an image.\n",
    "\n",
    "    :param input: The image to display.\n",
    "    :type input: numpy.ndarray\n",
    "    \n",
    "    :param title: Title of the image.\n",
    "    :type title: str\n",
    "\n",
    "    :param axis: Whether to show the axis or not.\n",
    "    :type axis: bool\n",
    "    \"\"\"\n",
    "    plt.imshow(input)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    if not axis:\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def get_optimal_workers(batch_size = 32, has_augmentation = True, conservative = False, verbose = True, use = \"recommended\"):\n",
    "    \"\"\"\n",
    "    Determine optimal number of workers for model.fit() based on system specs and use case.\n",
    "\n",
    "    :param batch_size: The batch size used for training. (Default: 32)\n",
    "    :type batch_size: int\n",
    "\n",
    "    :param has_augmentation: Whether the training process involves data augmentation. (Default: True)\n",
    "    :type has_augmentation: bool\n",
    "\n",
    "    :param conservative: Whether to use a conservative estimate for the number of workers. (Default: False)\n",
    "    :type conservative: bool\n",
    "\n",
    "    :param verbose: Whether to print recommendations and system info. (Default: True)\n",
    "    :type verbose: bool\n",
    "\n",
    "    :param use: The type of recommendation to use. Values are \"recommended\", \"current_default\", \"conservative\", \"aggressive\", or \"ask\". (Default: \"recommended\")\n",
    "    :type use: str\n",
    "\n",
    "    :return: Recommended number of workers.\n",
    "    :rtype: int\n",
    "\n",
    "    :raises ValueError: If `batch_size` is not a positive integer. Or if `use` is not one of the expected values.\n",
    "    \"\"\"\n",
    "    if not isinstance(batch_size, int) or batch_size <= 0:\n",
    "        raise ValueError(\"batch_size must be a positive integer\")\n",
    "\n",
    "    if use not in [\"recommended\", \"current_default\", \"conservative\", \"aggressive\", \"ask\"]:\n",
    "        raise ValueError(\"use must be one of: 'recommended', 'current_default', 'conservative', 'aggressive', or 'ask'\")\n",
    "    \n",
    "    # Get system info\n",
    "    # Includes hyperthreading\n",
    "    logical_cores = psutil.cpu_count(logical = True)\n",
    "    # Actual CPU cores\n",
    "    physical_cores = psutil.cpu_count(logical = False)\n",
    "    available_memory_gb = psutil.virtual_memory().total / (1024**3)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"System: {physical_cores} physical cores, {logical_cores} logical cores, {available_memory_gb:.1f}GB RAM\")\n",
    "    \n",
    "    # If conservative mode, leave one core for the system\n",
    "    if conservative:\n",
    "        recommended = max(1, physical_cores - 1)\n",
    "        reason = \"Conservative (leaves 1 core for system)\"\n",
    "    # If using GPU, adjust based on GPU memory\n",
    "    elif has_augmentation:\n",
    "        if batch_size <= 32:\n",
    "            recommended = physical_cores\n",
    "            reason = \"I/O-bound augmentation with small batches\"\n",
    "        elif batch_size <= 128:\n",
    "            recommended = min(logical_cores, physical_cores + 2)\n",
    "            reason = \"I/O-bound augmentation with medium batches\"\n",
    "        else:\n",
    "            recommended = logical_cores\n",
    "            reason = \"I/O-bound augmentation with large batches\"\n",
    "    # If not using augmentation, adjust based on batch size\n",
    "    else:\n",
    "        if batch_size <= 64:\n",
    "            recommended = max(1, physical_cores // 2)\n",
    "            reason = \"CPU-bound without augmentation\"\n",
    "        else:\n",
    "            recommended = physical_cores\n",
    "            reason = \"CPU-bound without augmentation, large batches\"\n",
    "    \n",
    "    # Memory constraint check (assume ~200MB per worker)\n",
    "    memory_limit = max(1, int(available_memory_gb * 1024 / 200))\n",
    "    if recommended > memory_limit:\n",
    "        recommended = memory_limit\n",
    "        reason += f\" (limited by {available_memory_gb:.1f}GB RAM)\"\n",
    "    \n",
    "    # Cap at reasonable maximum to prevent overhead\n",
    "    recommended = min(recommended, 16)\n",
    "\n",
    "    alts = {\n",
    "        \"Current default\": multiprocessing.cpu_count(),\n",
    "        \"Conservative\": max(1, physical_cores - 1),\n",
    "        \"Aggressive\": logical_cores,\n",
    "        \"Recommended\": recommended\n",
    "    }\n",
    "\n",
    "    alt_keys = {}\n",
    "    index = 0\n",
    "    for key in alts.keys():\n",
    "        index += 1\n",
    "        alt_keys[index] = key\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Recommended workers: {recommended} ({reason})\")\n",
    "        \n",
    "        # Show alternatives\n",
    "        print(\"Alternatives to test:\")\n",
    "        for name, count in alts.items():\n",
    "            if count != recommended:\n",
    "                print(f\"  {name}: {count} workers\")\n",
    "\n",
    "    use = use.lower().strip().replace(\" \", \"_\")\n",
    "\n",
    "    if use == \"current_default\":\n",
    "        to_return = alts[\"Current default\"]\n",
    "    elif use == \"conservative\":\n",
    "        to_return = alts[\"Conservative\"]\n",
    "    elif use == \"aggressive\":\n",
    "        to_return = alts[\"Aggressive\"]\n",
    "    elif use == \"ask\" and verbose:\n",
    "        while True:\n",
    "            print(\"\\nChoose a recommendation:\")\n",
    "            for index, key in alt_keys.items():\n",
    "                print(f\"  [{index}] {key}\")\n",
    "            choice = input(\"Enter the number of your choice: \").strip()\n",
    "\n",
    "            if choice.isdigit() and int(choice) in alt_keys:\n",
    "                to_return = alts[alt_keys[int(choice)]]\n",
    "                break\n",
    "            else:\n",
    "                print(\"Invalid choice, please try again.\")\n",
    "    else:\n",
    "        to_return = recommended\n",
    "        \n",
    "    return to_return\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a86cdc4",
   "metadata": {},
   "source": [
    "##### Logger Related Functions\n",
    "\n",
    "The `log_metrics` function logs all the needed metrics for the run. This includes CPU, GPU, and training metrics. The list below will explain all log keys in detail (aside from training metric keys).\n",
    "\n",
    "**Training Metrics**\n",
    "- `epoch`: The epoch in which the metrics were recorded.\n",
    "- `loss`: The loss value at the end of the epoch.\n",
    "- `accuracy`: The accuracy value at the end of the epoch.\n",
    "- `val_loss`: The validation loss value at the end of the epoch.\n",
    "- `val_accuracy`: The validation accuracy value at the end of the epoch.\n",
    "- `training_time_hours`: The total training time of the model.\n",
    "- `platform`: The platform in which the training is counducted (i.e., Intel i5, RTX 3050 Ti, Colab, ...)\n",
    "- `lr_strategy`: The learning rate strategy. Not really limited to learning rate and in this case, it could be the `EarlyStopping`, `ReduceLROnPlateau`, `None`, or `EarlyStopping-ReduceLROnPlateau`.\n",
    "- `env_temp`: Defines the environment (ambiance) temperature. A hardocded one must be provided (in the variable section) but if there's an API key from [WeatherAPI](weatherapi.com) is provided, then it fetches the `location`'s temperature every epoch end.\n",
    "- `timestamp`: The datetime when the epoch ended and recorded.\n",
    "\n",
    "**CPU Metrics**\n",
    "- `cpu_percent_peak`: The peak CPU usage throughout the entire epoch.\n",
    "- `cpu_percent`: Defines the average CPU usage for the entire epoch. The CPU usage also includes other running tasks and applications as isolating the usage of the training alone is practically impossible.\n",
    "- `cpu_power`: The power consumption of the CPU. This is calculated based on the provided `cpu_percent` using this formula: `(cpu_percent / 100) * cpu_tdp`.\n",
    "\n",
    "~~**GPU Metrics**~~\n",
    "- ~~`gpu_load`: The GPU's average load (in percentage) throughout the entire epoch.~~\n",
    "- ~~`gpu_load_peak`: The peak GPU load (in percentage) throughout the entire epoch.~~\n",
    "- ~~`gpu_power`: The GPU's average power consumption (in watts). This is directly drawn using the `nvidia-smi` command.~~\n",
    "- ~~`gpu_power_peak`: The peak GPU power consumption (in watts) during training. Similarly, it is also directly drawn using the `nvidia-smi` command.~~\n",
    "- ~~`gpu_temp`: The GPU temperature (in Celsius) during training (if available). This is directly drawn using the `nvidia-smi` command.~~\n",
    "- ~~`gpu_temp_peak`: The peak GPU temperature (in Celsius) during training (if available). Similarly, it is also directly drawn using the `nvidia-smi` command.~~\n",
    "\n",
    "**Efficiency Metrics**\n",
    "The `cost_local` defines the tester's current local currency. From that, the `cost_usd` is derived using `cost_local / to_usd_conversion`.\n",
    "\n",
    "- `cost_efficiency_local`: The cost efficiency in the local currency (calculated based on accuracy and cost). To be precise, it is calculated using the formula `(accuracy * 100) / cost_local`.\n",
    "- `cost_per_accuracy_point_local`: The cost per accuracy point in the local currency (calculated based on accuracy and cost). To be precise, it is calculated using the formula `cost_local / (accuracy * 100)`.\n",
    "- `cost_efficiency_usd`: The cost efficiency in US dollars (calculated based on accuracy and cost). To be precise, it is calculated using the formula `(accuracy * 100) / cost_usd`.\n",
    "- `cost_per_accuracy_point_usd`: The cost per accuracy point in US dollars (calculated based on accuracy and cost). To be precise, it is calculated using the formula `cost_usd / (accuracy * 100)`.\n",
    "- `power_efficiency`: The accuracy % per watt. Higher values shows more efficient power usage whereas lower ones are the opposite.\n",
    "- `time_efficiency`: The accuracy % per hour. Higher values shows more efficient power usage whereas lower ones are the opposite.\n",
    "\n",
    "Both the `cost_efficiency_local` and `cost_efficiency_usd` uses accuracy % per cost. Higher cost efficiency means better.\n",
    "\n",
    "On the other hand, `cost_per_accuracy_point_local` and `cost_per_accuracy_point_usd` uses cost per 1% accuracy; a direct opposite of the former."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d26b712",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_metrics(metrics: Dict[str, Any], platform: str, lr_strategy: str, callback_instance: Union[Callback, None] = None):\n",
    "    \"\"\"\n",
    "    Logs the training metrics to a JSON file, including the platform (manual input) and\n",
    "    learning rate strategy (manual input). Some metrics are fixed and are defined in the\n",
    "    global variables section (under `Metrics related variables`).\n",
    "\n",
    "    The metrics that are logged include:\n",
    "    - `epoch`: The epoch in which the metrics were recorded.\n",
    "    - `loss`: The loss value at the end of the epoch.\n",
    "    - `accuracy`: The accuracy value at the end of the epoch.\n",
    "    - `val_loss`: The validation loss value at the end of the epoch.\n",
    "    - `val_accuracy`: The validation accuracy value at the end of the epoch.\n",
    "    - `training_time_hours`: The total training time in hours.\n",
    "    - `platform`: The platform on which the training was performed (manual input).\n",
    "    - `lr_strategy`: The learning rate strategy used during training (manual input).\n",
    "    - `env_temp`: The room temperature during training (under `Metrics related variables`).\n",
    "    - `cpu_percent_peak`: The peak CPU usage (in percentage) during training.\n",
    "    - `cpu_percent`: The CPU usage (in percentage) during training.\n",
    "    - `cpu_power`: The CPU power consumption (in watts) during training.\n",
    "    - `gpu_load`: The GPU load (in percentage) during training.\n",
    "    - `gpu_load_peak`: The peak GPU load (in percentage) during training.\n",
    "    - `gpu_power`: The GPU power consumption (in watts) during training.\n",
    "    - `gpu_power_peak`: The peak GPU power consumption (in watts) during training.\n",
    "    - `gpu_temp`: The GPU temperature during training (if available).\n",
    "    - `gpu_temp_peak`: The peak GPU temperature during training (if available).\n",
    "    - `cost_efficiency_local`: The cost efficiency in the local currency (calculated based on accuracy and cost).\n",
    "    - `cost_per_accuracy_point_local`: The cost per accuracy point in the local currency (calculated based on accuracy and cost).\n",
    "    - `cost_efficiency_usd`: The cost efficiency in US dollars (calculated based on accuracy and cost).\n",
    "    - `cost_per_accuracy_point_usd`: The cost per accuracy point in US dollars (calculated based on accuracy and cost).\n",
    "    - `power_efficiency`: The efficiency in terms of accuracy per watt.\n",
    "    - `time_efficiency`: The efficiency in terms of accuracy per hour.\n",
    "    - `timestamp`: The timestamp when the metrics were logged.\n",
    "\n",
    "    **NOTE:** The `..._local_currency` refers to the local currency used by the researcher, which is defined in the global variables section.\n",
    "    This may be changed to a different currency to reflect the local currency of the researcher.\n",
    "    In the original experiment, the local currency is the Philippine peso (PHP). The variable is named `cost_per_kwh_local` so that it is easy\n",
    "    to understand that this could be changed easily into other local currency anyone wishes to use.\n",
    "\n",
    "    :param metrics: The training metrics to log.\n",
    "    :type metrics: dict\n",
    "\n",
    "    :param platform: The platform on which the training was performed.\n",
    "    :type platform: str\n",
    "\n",
    "    :param lr_strategy: The learning rate strategy used during training.\n",
    "    :type lr_strategy: str\n",
    "\n",
    "    :param callback_instance: The callback instance that triggered the logging, if any.\n",
    "    :type callback_instance: tensorflow.keras.callbacks.Callback or None\n",
    "    \"\"\"\n",
    "    timestamp = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n",
    "\n",
    "    # Add new fields to the metrics dictionary\n",
    "    metrics['platform'] = platform\n",
    "    metrics['lr_strategy'] = lr_strategy\n",
    "    \n",
    "    # Get the location's temperature\n",
    "    metrics['env_temp'] = get_env_temp()\n",
    "\n",
    "    get_cpu_metrics(metrics, callback_instance)\n",
    "    get_gpu_metrics(metrics, callback_instance)\n",
    "\n",
    "    # Other metrics\n",
    "    effective_power = {\n",
    "        \"cpu\": metrics.get(\"cpu_power\", 0),\n",
    "        \"gpu\": metrics.get(\"gpu_power\", 0)\n",
    "    }\n",
    "\n",
    "    if type(effective_power[\"cpu\"]) not in (int, float):\n",
    "        effective_power[\"cpu\"] = 0\n",
    "    if type(effective_power[\"gpu\"]) not in (int, float):\n",
    "        effective_power[\"gpu\"] = 0\n",
    "\n",
    "    # Efficiency metrics\n",
    "    total_power = effective_power[\"cpu\"] + effective_power[\"gpu\"]\n",
    "    get_efficiency_metrics(metrics, total_power)\n",
    "\n",
    "    metrics['timestamp'] = timestamp      \n",
    "\n",
    "    with open(file_name(file_postfix), \"a\") as log:\n",
    "        log.write(json.dumps(metrics) + \"\\n\")\n",
    "\n",
    "    print(metrics)\n",
    "\n",
    "def get_cpu_metrics(metrics: Dict[str, Any], callback_instance: Union[Callback, None] = None):\n",
    "    \"\"\"\n",
    "    Gets the CPU metrics and adds them to the metrics dictionary.\n",
    "\n",
    "    :param metrics: The training metrics to log.\n",
    "    :type metrics: dict\n",
    "\n",
    "    :param callback_instance: The callback instance that triggered the logging, if any.\n",
    "    :type callback_instance: tensorflow.keras.callbacks.Callback or None\n",
    "    \"\"\"\n",
    "    avg_cpu = None\n",
    "    peak_cpu = None\n",
    "    \n",
    "    try:\n",
    "        if callback_instance and hasattr(callback_instance, 'cpu_samples') and callback_instance.cpu_samples:\n",
    "            avg_cpu = callback_instance.get_average_cpu_usage()\n",
    "            peak_cpu = callback_instance.get_peak_cpu_usage()\n",
    "        else:\n",
    "            print(\"Warning: No CPU monitoring data available\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting CPU monitoring data: {e}\")\n",
    "        avg_cpu = None\n",
    "        peak_cpu = None\n",
    "\n",
    "    # Set CPU metrics\n",
    "    metrics[\"cpu_percent_peak\"] = round(peak_cpu, 2) if peak_cpu is not None else \"N/A\"\n",
    "    metrics[\"cpu_percent\"] = round(avg_cpu, 2) if avg_cpu is not None else \"N/A\"\n",
    "    \n",
    "    # Calculate CPU power (with validation)\n",
    "    if isinstance(metrics[\"cpu_percent\"], (int, float)) and metrics[\"cpu_percent\"] > 0:\n",
    "        metrics[\"cpu_power\"] = (metrics[\"cpu_percent\"] / 100) * cpu_tdp\n",
    "    else:\n",
    "        metrics[\"cpu_power\"] = \"N/A\"\n",
    "\n",
    "def get_gpu_metrics(metrics: Dict[str, Any], callback_instance: Union[Callback, None] = None):\n",
    "    \"\"\"\n",
    "    Gets the GPU metrics and adds them to the metrics dictionary.\n",
    "\n",
    "    :param metrics: The training metrics to log.\n",
    "    :type metrics: dict\n",
    "\n",
    "    :param callback_instance: The callback instance that triggered the logging, if any.\n",
    "    :type callback_instance: tensorflow.keras.callbacks.Callback or None\n",
    "    \"\"\"\n",
    "    metrics[\"gpu_load\"] = \"N/A\"\n",
    "    metrics[\"gpu_load_peak\"] = \"N/A\"\n",
    "    metrics[\"gpu_power\"] = \"N/A\"\n",
    "    metrics[\"gpu_power_peak\"] = \"N/A\"\n",
    "    metrics[\"gpu_temp\"] = \"N/A\"\n",
    "    metrics[\"gpu_temp_peak\"] = \"N/A\"\n",
    "\n",
    "    if gputil_exist:\n",
    "        try:\n",
    "            # Try to get monitored data from callback first\n",
    "            if callback_instance and hasattr(callback_instance, 'gpu_load_samples'):\n",
    "                avg_gpu_load = callback_instance.get_average_gpu_load()\n",
    "                peak_gpu_load = callback_instance.get_peak_gpu_load()\n",
    "                avg_gpu_power = callback_instance.get_average_gpu_power()\n",
    "                peak_gpu_power = callback_instance.get_peak_gpu_power()\n",
    "                avg_gpu_temp = callback_instance.get_average_gpu_temp()\n",
    "                peak_gpu_temp = callback_instance.get_peak_gpu_temp()\n",
    "\n",
    "                # Set GPU load metrics\n",
    "                if avg_gpu_load is not None:\n",
    "                    metrics[\"gpu_load\"] = round(avg_gpu_load, 2)\n",
    "                if peak_gpu_load is not None:\n",
    "                    metrics[\"gpu_load_peak\"] = round(peak_gpu_load, 2)\n",
    "\n",
    "                # Set GPU power metrics\n",
    "                if avg_gpu_power is not None:\n",
    "                    metrics[\"gpu_power\"] = round(avg_gpu_power, 2)\n",
    "                if peak_gpu_power is not None:\n",
    "                    metrics[\"gpu_power_peak\"] = round(peak_gpu_power, 2)\n",
    "\n",
    "                # Set GPU temperature metrics\n",
    "                if avg_gpu_temp is not None:\n",
    "                    metrics[\"gpu_temp\"] = round(avg_gpu_temp, 2)\n",
    "                if peak_gpu_temp is not None:\n",
    "                    metrics[\"gpu_temp_peak\"] = round(peak_gpu_temp, 2)\n",
    "\n",
    "            else:\n",
    "                # Fallback to snapshot method if no monitoring data available\n",
    "                print(\"Warning: No GPU monitoring data available, using snapshot method\")\n",
    "                gpus = GPUtil.getGPUs()\n",
    "\n",
    "                if gpus:\n",
    "                    metrics[\"gpu_load\"] = round(gpus[0].load * 100, 2)  # Convert to percentage\n",
    "\n",
    "                    try:\n",
    "                        # Attempt to get GPU power using nvidia-smi\n",
    "                        command_power = \"nvidia-smi --query-gpu=power.draw --format=csv,noheader,nounits\"\n",
    "                        output = sp.check_output(command_power, shell=True).decode('utf-8').strip()\n",
    "                        metrics[\"gpu_power\"] = round(float(output), 2) if output else 0.0\n",
    "\n",
    "                        # Attempt to get GPU temperature\n",
    "                        command_temp = \"nvidia-smi --query-gpu=temperature.gpu --format=csv,noheader,nounits\"\n",
    "                        output_temp = sp.check_output(command_temp, shell=True).decode('utf-8').strip()\n",
    "                        metrics[\"gpu_temp\"] = round(float(output_temp), 2) if output_temp else \"N/A\"\n",
    "                    except (sp.CalledProcessError, FileNotFoundError):\n",
    "                        pass  # Keep default \"N/A\" values\n",
    "                    except Exception:\n",
    "                        metrics[\"gpu_power\"] = \"Error\"\n",
    "                        metrics[\"gpu_temp\"] = \"Error\"\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting GPU metrics: {e}\")\n",
    "            for key in [\"gpu_load\", \"gpu_load_peak\", \"gpu_power\", \"gpu_power_peak\", \"gpu_temp\", \"gpu_temp_peak\"]:\n",
    "                if metrics[key] == \"N/A\":\n",
    "                    metrics[key] = \"Error\"\n",
    "\n",
    "def get_efficiency_metrics(metrics: Dict[str, Any], total_power: float):\n",
    "    \"\"\"\n",
    "    Calculates the efficiency metrics and adds them to the metrics dictionary.\n",
    "\n",
    "    :param metrics: The training metrics to log.\n",
    "    :type metrics: dict\n",
    "    \"\"\"\n",
    "    accuracy = metrics.get(\"accuracy\", 0)\n",
    "    training_time_hours = metrics.get(\"training_time_hours\", 0)\n",
    "\n",
    "    energy_kwh = (total_power / 1000) * training_time_hours\n",
    "    cost_local = energy_kwh * cost_per_kwh_local\n",
    "    cost_usd = cost_local / to_usd_conversion\n",
    "\n",
    "    metrics['cost_efficiency_local'] = \"N/A\"\n",
    "    metrics['cost_per_accuracy_point_local'] = \"N/A\"\n",
    "    metrics['cost_efficiency_usd'] = \"N/A\"\n",
    "    metrics['cost_per_accuracy_point_usd'] = \"N/A\"\n",
    "        \n",
    "    if cost_local > 0:\n",
    "        # Accuracy % per local currency\n",
    "        metrics[\"cost_efficiency_local\"] = (accuracy * 100) / cost_local\n",
    "        # Local currency per 1% accuracy\n",
    "        metrics['cost_per_accuracy_point_local'] = cost_local / (accuracy * 100)\n",
    "\n",
    "    if cost_usd > 0:\n",
    "        # Accuracy % per dollar\n",
    "        metrics[\"cost_efficiency_usd\"] = (accuracy * 100) / cost_usd\n",
    "        # Dollar per 1% accuracy\n",
    "        metrics['cost_per_accuracy_point_usd'] = cost_usd / (accuracy * 100)\n",
    "\n",
    "    if total_power > 0:\n",
    "        # Accuracy % per watt\n",
    "        metrics['power_efficiency'] = (accuracy * 100) / total_power\n",
    "\n",
    "    if training_time_hours > 0:\n",
    "        # Accuracy % per hour\n",
    "        metrics['time_efficiency'] = (accuracy * 100) / training_time_hours\n",
    "\n",
    "def get_env_temp():\n",
    "    \"\"\"\n",
    "    Fetches the environment temperature from a weather API if available. If the API key is not\n",
    "    set or the request fails, it returns a default temperature value defined in the global\n",
    "    variable `env_temp`.\n",
    "\n",
    "    :return: The environment temperature in degrees Celsius.\n",
    "    :rtype: float\n",
    "    \"\"\"\n",
    "\n",
    "    # Uses a global variable to store the environment temperature.\n",
    "    global env_temp\n",
    "\n",
    "    # Set the environment temperature when API key is available in the .env file.\n",
    "    if os.path.exists(\"./../.env\"):\n",
    "        from dotenv import load_dotenv\n",
    "        load_dotenv(\"./../.env\")\n",
    "\n",
    "    if \"WEATHER_API\" in os.environ and os.environ[\"WEATHER_API\"] is not None and len(os.environ[\"WEATHER_API\"]) > 0:\n",
    "        from requests import get\n",
    "\n",
    "        # Uses `longitude,latitude` format. For more input formats, refer to the\n",
    "        # WeatherAPI documentation (https://www.weatherapi.com/docs/#intro-request).\n",
    "        location = \"14.57,121.14\" # Rizal, Philippines\n",
    "        key = os.environ[\"WEATHER_API\"]\n",
    "        try:\n",
    "            obj = get(f\"http://api.weatherapi.com/v1/current.json?key={key}&q={location}\")\n",
    "\n",
    "            if obj.status_code == 200:\n",
    "                obj = obj.json()\n",
    "                env_temp = obj[\"current\"][\"temp_c\"]\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching weather data; using default temperature ({env_temp}°C).\")\n",
    "            print(e)\n",
    "\n",
    "    return env_temp\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053d28c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom callback class to log the metrics real-time during training.\n",
    "class MetricLoggerCallback(Callback):\n",
    "    \"\"\"\n",
    "    A custom callback to log training metrics and system performance during the training process,\n",
    "    allowing the researcher to monitor crucial statistics for the experiment.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, platform: str, lr_strategy: str, start_time: Union[float, None] = None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.platform = platform\n",
    "        self.lr_strategy = lr_strategy\n",
    "        self.start_time_session = start_time if start_time is not None else time.time()\n",
    "\n",
    "        # CPU monitoring attributes\n",
    "        self.cpu_samples = deque(maxlen = 1000)\n",
    "        self.monitoring_thread = None\n",
    "        self.stop_monitoring = threading.Event()\n",
    "        self.monitoring_active = False\n",
    "\n",
    "        # GPU monitoring attributes\n",
    "        self.gpu_load_samples = deque(maxlen = 1000)\n",
    "        self.gpu_power_samples = deque(maxlen = 1000)\n",
    "        self.gpu_temp_samples = deque(maxlen = 1000)\n",
    "        self.gpu_monitoring_thread = None\n",
    "        self.stop_gpu_monitoring_event = threading.Event()\n",
    "        self.gpu_monitoring_active = False\n",
    "\n",
    "    def start_cpu_monitoring(self):\n",
    "        \"\"\"\n",
    "        Starts a background thread to monitor CPU usage and power consumption.\n",
    "        This method is called when the training starts.\n",
    "        \"\"\"\n",
    "        if not self.monitoring_active:\n",
    "            self.stop_monitoring.clear()\n",
    "            self.monitoring_thread = threading.Thread(target=self._monitor_cpu, daemon=True)\n",
    "            self.monitoring_thread.start()\n",
    "            self.monitoring_active = True\n",
    "\n",
    "    def stop_cpu_monitoring(self):\n",
    "        \"\"\"\n",
    "        Stops the background thread that monitors CPU usage and power consumption.\n",
    "        This method is called when the training ends.\n",
    "        \"\"\"\n",
    "        if self.monitoring_active:\n",
    "            self.stop_monitoring.set()\n",
    "            if self.monitoring_thread:\n",
    "                self.monitoring_thread.join(timeout=2)\n",
    "            self.monitoring_active = False\n",
    "            print(\"CPU monitoring stopped\")\n",
    "\n",
    "    def start_gpu_monitoring(self):\n",
    "        \"\"\"\n",
    "        Starts a background thread to monitor GPU usage, power consumption, and temperature.\n",
    "        This method is called when the training starts.\n",
    "        \"\"\"\n",
    "        if not self.gpu_monitoring_active and gputil_exist:\n",
    "            self.stop_gpu_monitoring_event.clear()\n",
    "            self.gpu_monitoring_thread = threading.Thread(target=self._monitor_gpu, daemon=True)\n",
    "            self.gpu_monitoring_thread.start()\n",
    "            self.gpu_monitoring_active = True\n",
    "\n",
    "    def stop_gpu_monitoring(self):\n",
    "        \"\"\"\n",
    "        Stops the background thread that monitors GPU usage, power consumption, and temperature.\n",
    "        This method is called when the training ends.\n",
    "        \"\"\"\n",
    "        if self.gpu_monitoring_active:\n",
    "            self.stop_gpu_monitoring_event.set()\n",
    "            if self.gpu_monitoring_thread:\n",
    "                self.gpu_monitoring_thread.join(timeout=2)\n",
    "            self.gpu_monitoring_active = False\n",
    "            print(\"GPU monitoring stopped\")\n",
    "\n",
    "    def _monitor_cpu(self):\n",
    "        \"\"\"\n",
    "        Background method to sample CPU usage at regular intervals.\n",
    "        This method runs in a separate thread and continuously samples the CPU usage\n",
    "        until the `stop_monitoring` event is set.\n",
    "        \"\"\"\n",
    "        while not self.stop_monitoring.is_set():\n",
    "            try:\n",
    "                # Sample CPU usage with a 1-second interval\n",
    "                cpu_percent = psutil.cpu_percent(interval=1)\n",
    "                self.cpu_samples.append(cpu_percent)\n",
    "            except Exception as e:\n",
    "                print(f\"Error sampling CPU: {e}\")\n",
    "                break\n",
    "\n",
    "    def _monitor_gpu(self):\n",
    "        \"\"\"\n",
    "        Background method to sample GPU metrics at regular intervals.\n",
    "        This method runs in a separate thread and continuously samples GPU load, power, and temperature\n",
    "        until the `stop_gpu_monitoring` event is set.\n",
    "        \"\"\"\n",
    "        while not self.stop_gpu_monitoring_event.is_set():\n",
    "            try:\n",
    "                # Sample GPU metrics\n",
    "                gpus = GPUtil.getGPUs()\n",
    "                if gpus:\n",
    "                    # GPU load (utilization)\n",
    "                    gpu_load = gpus[0].load * 100  # Convert to percentage\n",
    "                    self.gpu_load_samples.append(gpu_load)\n",
    "\n",
    "                    # GPU power consumption via nvidia-smi\n",
    "                    try:\n",
    "                        command_power = \"nvidia-smi --query-gpu=power.draw --format=csv,noheader,nounits\"\n",
    "                        output = sp.check_output(command_power, shell=True).decode('utf-8').strip()\n",
    "                        gpu_power = float(output) if output else 0.0\n",
    "                        self.gpu_power_samples.append(gpu_power)\n",
    "                    except (sp.CalledProcessError, FileNotFoundError, ValueError):\n",
    "                        # If nvidia-smi fails, we still keep the load data\n",
    "                        pass\n",
    "\n",
    "                    # GPU temperature via nvidia-smi\n",
    "                    try:\n",
    "                        command_temp = \"nvidia-smi --query-gpu=temperature.gpu --format=csv,noheader,nounits\"\n",
    "                        output_temp = sp.check_output(command_temp, shell=True).decode('utf-8').strip()\n",
    "                        gpu_temp = float(output_temp) if output_temp else 0.0\n",
    "                        self.gpu_temp_samples.append(gpu_temp)\n",
    "                    except (sp.CalledProcessError, FileNotFoundError, ValueError):\n",
    "                        # If nvidia-smi fails, we still keep the load data\n",
    "                        pass\n",
    "\n",
    "                # Wait 1 second before next sample\n",
    "                time.sleep(1)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error sampling GPU: {e}\")\n",
    "                break\n",
    "\n",
    "    def get_average_cpu_usage(self):\n",
    "        \"\"\"\n",
    "        Calculate average CPU usage from collected samples.\n",
    "\n",
    "        :return: The average CPU usage as a percentage.\n",
    "        :rtype: float\n",
    "        \"\"\"\n",
    "        if not self.cpu_samples:\n",
    "            return 0\n",
    "        return sum(self.cpu_samples) / len(self.cpu_samples)\n",
    "\n",
    "    def get_peak_cpu_usage(self):\n",
    "        \"\"\"\n",
    "        Calculate peak CPU usage from collected samples.\n",
    "\n",
    "        :return: The peak CPU usage as a percentage.\n",
    "        :rtype: float\n",
    "        \"\"\"\n",
    "        if not self.cpu_samples:\n",
    "            return 0\n",
    "        return max(self.cpu_samples)\n",
    "\n",
    "    def get_average_gpu_load(self):\n",
    "        \"\"\"\n",
    "        Calculate average GPU load from collected samples.\n",
    "\n",
    "        :return: The average GPU load as a percentage.\n",
    "        :rtype: float\n",
    "        \"\"\"\n",
    "        if not self.gpu_load_samples:\n",
    "            return None\n",
    "        return sum(self.gpu_load_samples) / len(self.gpu_load_samples)\n",
    "\n",
    "    def get_peak_gpu_load(self):\n",
    "        \"\"\"\n",
    "        Calculate peak GPU load from collected samples.\n",
    "\n",
    "        :return: The peak GPU load as a percentage.\n",
    "        :rtype: float\n",
    "        \"\"\"\n",
    "        if not self.gpu_load_samples:\n",
    "            return None\n",
    "        return max(self.gpu_load_samples)\n",
    "\n",
    "    def get_average_gpu_power(self):\n",
    "        \"\"\"\n",
    "        Calculate average GPU power consumption from collected samples.\n",
    "\n",
    "        :return: The average GPU power consumption in watts.\n",
    "        :rtype: float\n",
    "        \"\"\"\n",
    "        if not self.gpu_power_samples:\n",
    "            return None\n",
    "        return sum(self.gpu_power_samples) / len(self.gpu_power_samples)\n",
    "\n",
    "    def get_peak_gpu_power(self):\n",
    "        \"\"\"\n",
    "        Calculate peak GPU power consumption from collected samples.\n",
    "\n",
    "        :return: The peak GPU power consumption in watts.\n",
    "        :rtype: float\n",
    "        \"\"\"\n",
    "        if not self.gpu_power_samples:\n",
    "            return None\n",
    "        return max(self.gpu_power_samples)\n",
    "\n",
    "    def get_average_gpu_temp(self):\n",
    "        \"\"\"\n",
    "        Calculate average GPU temperature from collected samples.\n",
    "\n",
    "        :return: The average GPU temperature in degrees Celsius.\n",
    "        :rtype: float\n",
    "        \"\"\"\n",
    "        if not self.gpu_temp_samples:\n",
    "            return None\n",
    "        return sum(self.gpu_temp_samples) / len(self.gpu_temp_samples)\n",
    "\n",
    "    def get_peak_gpu_temp(self):\n",
    "        \"\"\"\n",
    "        Calculate peak GPU temperature from collected samples.\n",
    "\n",
    "        :return: The peak GPU temperature in degrees Celsius.\n",
    "        :rtype: float\n",
    "        \"\"\"\n",
    "        if not self.gpu_temp_samples:\n",
    "            return None\n",
    "        return max(self.gpu_temp_samples)\n",
    "\n",
    "    def on_train_begin(self, logs: Union[dict, None] = None):\n",
    "        \"\"\"\n",
    "        Called at the beginning of training. This method initializes the CPU monitoring\n",
    "        and starts the background thread to monitor CPU usage and power consumption.\n",
    "\n",
    "        :param logs: A dictionary containing the logs for the training.\n",
    "        :type logs: dict or None\n",
    "        \"\"\"\n",
    "        self.start_cpu_monitoring()\n",
    "        self.start_gpu_monitoring()\n",
    "\n",
    "    def on_train_end(self, logs: Union[dict, None] = None):\n",
    "        \"\"\"\n",
    "        Called at the end of training. This method stops the CPU monitoring\n",
    "        and joins the background thread to ensure it has completed before the program exits.\n",
    "\n",
    "        :param logs: A dictionary containing the logs for the training.\n",
    "        :type logs: dict or None\n",
    "        \"\"\"\n",
    "        self.stop_cpu_monitoring()\n",
    "        self.stop_gpu_monitoring()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs = None):\n",
    "        \"\"\"\n",
    "        Called at the end of each epoch to log the metrics and system performance.\n",
    "        \n",
    "        :param epoch: The current epoch number.\n",
    "        :type epoch: int\n",
    "        \n",
    "        :param logs: A dictionary containing the metrics for the current epoch.\n",
    "        :type logs: dict\n",
    "        \"\"\"\n",
    "        logs = logs or {}\n",
    "\n",
    "        # Calculate cumulative training time up to this epoch\n",
    "        elapsed_time_seconds = time.time() - self.start_time_session\n",
    "        training_time_hours = elapsed_time_seconds / 3600\n",
    "\n",
    "        # Prepare metrics dictionary for the log_metrics function\n",
    "        epoch_metrics = {\n",
    "            \"epoch\": epoch + 1,  # Keras epochs are 0-indexed, so add 1 for logging\n",
    "            \"loss\": logs.get('loss'),\n",
    "            \"accuracy\": logs.get('accuracy'),\n",
    "            \"val_loss\": logs.get('val_loss'),\n",
    "            \"val_accuracy\": logs.get('val_accuracy'),\n",
    "            \"training_time_hours\": training_time_hours\n",
    "        }\n",
    "\n",
    "        log_metrics(\n",
    "            metrics = epoch_metrics,\n",
    "            lr_strategy = self.lr_strategy,\n",
    "            platform = self.platform,\n",
    "            callback_instance = self\n",
    "        )\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9f0061",
   "metadata": {},
   "source": [
    "##### Model Related Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ee9bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_block(x, filters, stride = 1):\n",
    "    \"\"\"\n",
    "    Defines a standard ResNet residual block with two convolutional layers.\n",
    "    Handles identity and projection shortcuts based on stride and filter changes.\n",
    "\n",
    "    :param x: Input tensor to the residual block.\n",
    "    :type x: tf.Tensor\n",
    "\n",
    "    :param filters: Number of filters for the convolutional layers within the block.\n",
    "    :type filters: int\n",
    "\n",
    "    :param stride: Number of pixels to skip when applying the convolutional layers.\n",
    "    :type stride: int\n",
    "\n",
    "    :return x: Output tensor after applying the residual block operations.\n",
    "    :rtype: tf.Tensor\n",
    "    \"\"\"\n",
    "    shortcut = x\n",
    "    \n",
    "    # Downsample if the stride is not 1 or if the number of filters in the shortcut does not match\n",
    "    if stride != 1 or shortcut.shape[-1] != filters:\n",
    "        shortcut = layers.Conv2D(filters, (1, 1), strides=stride, padding='valid', use_bias=False)(shortcut)\n",
    "        shortcut = layers.BatchNormalization()(shortcut)\n",
    "\n",
    "    # First convolutional layer in the residual path\n",
    "    x = layers.Conv2D(filters, (3, 3), strides=stride, padding='same', use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x) # ReLU after BN as per diagram for first conv\n",
    "\n",
    "    # Second convolutional layer in the residual path\n",
    "    x = layers.Conv2D(filters, (3, 3), strides=1, padding='same', use_bias=False)(x) # Second conv always stride 1\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    # Add the shortcut to the residual path output\n",
    "    x = layers.add([x, shortcut])\n",
    "    x = layers.ReLU()(x) # Final ReLU after addition for the block output\n",
    "    \n",
    "    return x\n",
    "\n",
    "def build_resnet18(input_shape:tuple = (32, 32, 3), num_classes:int = 10, model_name:str = \"ResNet18_CIFAR10_Functional\"):\n",
    "    \"\"\"\n",
    "    Builds the ResNet-18 model for CIFAR-10 classification using the Keras Functional API.\n",
    "\n",
    "    This implementation closely follows the provided ResNet-18 architecture diagram\n",
    "    (e.g., [resnet18_cifar10_diagram.png](./../resnet18_cifar10_diagram.png)) for CIFAR-10 specific configurations like initial\n",
    "    filter counts and inclusion of Dropout.\n",
    "    \"\"\"\n",
    "    input_tensor = Input(shape=input_shape)\n",
    "\n",
    "    # Initial Convolutional Layer\n",
    "    # Diagram: Conv2D (32, 3x3, stride=1)\n",
    "    x = layers.Conv2D(32, (3, 3), strides=(1, 1), padding='same', use_bias=False)(input_tensor)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "\n",
    "    # Stage 1: Two residual blocks with 64 filters, stride 1 (no downsampling)\n",
    "    # Diagram: Residual Block 1a (64->64), 1b (64->64)\n",
    "    x = residual_block(x, 64, stride=1) # Residual Block 1a\n",
    "    x = residual_block(x, 64, stride=1) # Residual Block 1b\n",
    "\n",
    "    # Stage 2: Two residual blocks with 128 filters, first block has stride 2 (downsampling)\n",
    "    # Diagram: Residual Block 2a (64->128, stride=2), 2b (128->128)\n",
    "    x = residual_block(x, 128, stride=2) # Residual Block 2a (downsamples spatial dims by 2)\n",
    "    x = residual_block(x, 128, stride=1) # Residual Block 2b\n",
    "\n",
    "    # Stage 3: Two residual blocks with 256 filters, first block has stride 2 (downsampling)\n",
    "    # Diagram: Residual Block 3a (128->256, stride=2), 3b (256->256)\n",
    "    x = residual_block(x, 256, stride=2) # Residual Block 3a (downsamples spatial dims by 2)\n",
    "    x = residual_block(x, 256, stride=1) # Residual Block 3b\n",
    "\n",
    "    # Stage 4: Two residual blocks with 512 filters, first block has stride 2 (downsampling)\n",
    "    # Diagram: Residual Block 4a (256->512, stride=2), 4b (512->512)\n",
    "    x = residual_block(x, 512, stride=2) # Residual Block 4a (downsamples spatial dims by 2)\n",
    "    x = residual_block(x, 512, stride=1) # Residual Block 4b\n",
    "\n",
    "    # Final Layers\n",
    "    # Diagram: AdaptiveAvgPool2D (4x4 -> 1x1) - GlobalAveragePooling2D is the Keras equivalent\n",
    "    x = layers.GlobalAveragePooling2D()(x) # Output shape will be (batch_size, 512)\n",
    "\n",
    "    # Diagram: Flatten (512,) - Redundant after GlobalAveragePooling2D, but included for explicit diagram steps\n",
    "    x = layers.Flatten()(x)\n",
    "\n",
    "    # Diagram: Dropout (p=0.5)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "\n",
    "    # Diagram: Dense (512 -> 10)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    # Create the Keras Model\n",
    "    model = Model(inputs = input_tensor, outputs = outputs, name = model_name)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583c5d14",
   "metadata": {},
   "source": [
    "#### Processes\n",
    "\n",
    "Some data processing like pixel normalization and such."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a006f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility. Starts with 42 then increment per experiment.\n",
    "# Change the `run_number` variable to increment the seed and imply the next run.\n",
    "# You can also change the seed value to any integer you want to serve as the base\n",
    "# seed.\n",
    "seed = 42 + (run_number - 1)\n",
    "\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb884308",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['train']['x'] = data['train']['x'].astype('float32') / 255.0\n",
    "data['test']['x'] = data['test']['x'].astype('float32') / 255.0\n",
    "\n",
    "data['train']['y'] = data['train']['y'].flatten()\n",
    "data['test']['y'] = data['test']['y'].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2397c57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_strats = \"\"\n",
    "\n",
    "for strat in callbacks:\n",
    "    if callbacks.index(strat) > 0:\n",
    "        lr_strats += \" & \"\n",
    "\n",
    "    lr_strats += strat.__class__.__name__\n",
    "\n",
    "callbacks.append(\n",
    "    MetricLoggerCallback(\n",
    "        platform = platform,\n",
    "        lr_strategy = lr_strats,\n",
    "        start_time = time.time()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c9ae61",
   "metadata": {},
   "source": [
    "#### Preview\n",
    "\n",
    "Preview the created and instantiated variables and functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1508f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train data shape: {data['train']['x'].shape}, Train labels shape: {data['train']['y'].shape}\")\n",
    "print(f\"Test data shape: {data['test']['x'].shape}, Test labels shape: {data['test']['y'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ce48ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = random.randint(0, len(data['train']['x']) - 1)\n",
    "showImg(data['train']['x'][target], title = f\"Label: {data['class_names'][data['train']['y'][target]]} ({data['train']['y'][target]})\", axis = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865adc56",
   "metadata": {},
   "source": [
    "## Process\n",
    "\n",
    "Here begins the process which includes data splitting and pre-processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeda29fc",
   "metadata": {},
   "source": [
    "### Splitting the Data\n",
    "\n",
    "The split is to allow a separate validation batch from the test batch that will be used later for the `model.predict()` function. for getting the unbiased accuracy and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb39c6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training data into training and validation sets.\n",
    "data['train']['x'], data['val']['x'], data['train']['y'], data['val']['y'] = train_test_split(\n",
    "    data['train']['x'], data['train']['y'],\n",
    "    test_size = 0.2,\n",
    "    random_state = seed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a1326e",
   "metadata": {},
   "source": [
    "### Pre-Processing\n",
    "\n",
    "Image pre-processing, along with some image modification (skew, translate, etc.) are done here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9a8d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Data\n",
    "data['train']['generator'] = (ImageDataGenerator(\n",
    "\trescale = 1./255,\n",
    "    rotation_range = 20,\n",
    "    width_shift_range = 0.2,\n",
    "    height_shift_range = 0.2,\n",
    "    shear_range = 0.2,\n",
    "    zoom_range = 0.2,\n",
    "    horizontal_flip = True\n",
    ")).flow(\n",
    "\tdata['train']['x'],\n",
    "\tdata['train']['y'],\n",
    "    seed = seed,\n",
    ")\n",
    "\n",
    "# Validation Data\n",
    "data['val']['generator'] = ImageDataGenerator(\n",
    "    rescale = 1./255\n",
    ").flow(\n",
    "    data['val']['x'],\n",
    "    data['val']['y'],\n",
    "    seed = seed,\n",
    ")\n",
    "\n",
    "# Test Data\n",
    "data['test']['generator'] = (ImageDataGenerator(\n",
    "    rescale = 1./255,\n",
    ")).flow(\n",
    "\tdata['test']['x'],\n",
    "\tdata['test']['y'],\n",
    "    shuffle = False,\n",
    "    seed = seed,\n",
    ")\n",
    "\n",
    "print(f\"Train data generator: {data['train']['generator'].n} samples, {data['train']['generator'].batch_size} batch size\")\n",
    "print(f\"Validation data generator: {data['val']['generator'].n} samples, {data['val']['generator'].batch_size} batch size\")\n",
    "print(f\"Test data generator: {data['test']['generator'].n} samples, {data['test']['generator'].batch_size} batch size\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2546ca3",
   "metadata": {},
   "source": [
    "### Creating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e23c0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_resnet18()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c4461f",
   "metadata": {},
   "source": [
    "### Compilation and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec47e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer = Adam(learning_rate = 1e-4), # 0.0001\n",
    "    loss = 'sparse_categorical_crossentropy',\n",
    "    metrics = ['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f44a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "SPE = len(data['train']['x']) // batch_size\n",
    "VS = len(data['test']['x']) // batch_size\n",
    "\n",
    "print(f\"Steps per epoch: {SPE}, Validation steps: {VS} (batch size: {batch_size})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5001f50",
   "metadata": {},
   "source": [
    "Alternative keys could be:\n",
    "\n",
    "- `recommended`: The default value.\n",
    "- `current_default`: Uses the CPU count via `multiprocessing.cpu_count()` function.\n",
    "- `conservative`: The lowest possible.\n",
    "- `aggressive`: The highest possible.\n",
    "- `ask`: Prompts which key to use at the end of the run so you can check which you want to use. Must have `verbose` as `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b96594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: 6 physical cores, 12 logical cores, 31.8GB RAM\n",
      "Recommended workers: 8 (I/O-bound augmentation with medium batches)\n",
      "Alternatives to test:\n",
      "  Current default: 12 workers\n",
      "  Conservative: 5 workers\n",
      "  Aggressive: 12 workers\n",
      "\n",
      "Using 8 workers for training.\n"
     ]
    }
   ],
   "source": [
    "optimal_worker = get_optimal_workers(\n",
    "    batch_size = batch_size,\n",
    "    has_augmentation = True,\n",
    "    verbose = True,\n",
    "    use = 'aggressive'\n",
    ")\n",
    "\n",
    "optimal_worker = int(optimal_worker)\n",
    "print(f\"\\nUsing {optimal_worker} workers for training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb423fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    data[\"train\"][\"generator\"],\n",
    "    steps_per_epoch = SPE,\n",
    "    epochs = 100,\n",
    "    validation_data = data[\"val\"][\"generator\"],\n",
    "    validation_steps = VS,\n",
    "    callbacks = callbacks,\n",
    "\n",
    "    # Multiprocessing settings - use_multiprocessing is set to False due to issues when used in Windows native.\n",
    "    # use_multiprocessing = True,\n",
    "    workers = optimal_worker,\n",
    "    max_queue_size = 20,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0812c322",
   "metadata": {},
   "source": [
    "## Model Metrics Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e1e485",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, accuracy = model.evaluate(\n",
    "    data[\"test\"][\"generator\"],\n",
    "    steps = VS,\n",
    "    verbose = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be86d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(\n",
    "    data['test']['generator'],\n",
    "    steps = VS,\n",
    "    verbose = 1,\n",
    ")\n",
    "\n",
    "y_pred = np.argmax(y_pred, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c074e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(y_pred))\n",
    "\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6829d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = []\n",
    "for _ in range(VS):\n",
    "    _, labels = next(data[\"test\"][\"generator\"])\n",
    "    \n",
    "    if labels.ndim > 1 and labels.shape[1] > 1:\n",
    "        labels = np.argmax(labels, axis=1)\n",
    "    y_true.extend(labels)\n",
    "\n",
    "y_true = np.array(y_true)\n",
    "\n",
    "if len(y_true) != len(y_pred):\n",
    "    min_len = min(len(y_true), len(y_pred))\n",
    "    y_true = y_true[:min_len]\n",
    "    y_pred = y_pred[:min_len]\n",
    "\n",
    "print(len(y_true))\n",
    "\n",
    "y_true.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e680ea",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2efc2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "accuracy *= 100\n",
    "\n",
    "print(f\"Accuracy (Updated): {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f498609e",
   "metadata": {},
   "outputs": [],
   "source": [
    "unixTime = int(datetime.datetime.now().timestamp() * 1e6)\n",
    "\n",
    "plt.figure(figsize = (10, 6))\n",
    "plt.plot(history.history['accuracy'], color = 'blue', label = 'train')\n",
    "plt.plot(history.history['val_accuracy'], color = 'red', label = 'val')\n",
    "plt.plot(history.history['loss'], color = 'green', label = 'train loss')\n",
    "plt.plot(history.history['val_loss'], color = 'orange', label = 'val loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title(f'Accuracy over epochs\\n{platform} - {lr_strats}\\n{data[\"train\"][\"generator\"].batch_size} batch size')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "if not os.path.exists(f\"{prefix}/accuracy\"):\n",
    "    os.makedirs(f\"{prefix}/accuracy\")\n",
    "\n",
    "if save_plots:\n",
    "    plt.savefig(f\"{prefix}/accuracy/{platform}-{unixTime}-{accuracy:.2f}%.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c81f94",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37428fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(\n",
    "    y_true,\n",
    "    y_pred\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot = True, fmt = 'd', cmap = 'Blues')\n",
    "\n",
    "plt.title(f'Confusion Matrix \\n{platform} - {lr_strats}\\n{data[\"train\"][\"generator\"].batch_size} batch size')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "\n",
    "if not os.path.exists(f\"{prefix}/confusion_matrix\"):\n",
    "    os.makedirs(f\"{prefix}/confusion_matrix\")\n",
    "\n",
    "if save_plots:\n",
    "    plt.savefig(f\"{prefix}/confusion_matrix/{platform}-{unixTime}-{accuracy:.2f}%.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ffca80",
   "metadata": {},
   "source": [
    "## Notification Sound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004d854c",
   "metadata": {},
   "outputs": [],
   "source": [
    "alarm_iteration = 100\n",
    "alarm_sound = \"notification.wav\"\n",
    "done_sound = \"done.wav\"\n",
    "\n",
    "try:\n",
    "    if os.name == 'nt':\n",
    "        for i in range(alarm_iteration):\n",
    "            delay = float(\"{0:.2f}\".format(random.uniform(0, 0.5)))\n",
    "            \n",
    "            if os.path.exists(alarm_sound):\n",
    "                winsound.PlaySound(alarm_sound, winsound.SND_FILENAME)\n",
    "                time.sleep(delay)\n",
    "            else:\n",
    "                winsound.Beep(1000, 500)\n",
    "                time.sleep(0.5)\n",
    "\n",
    "            print(f\"Alarm sound played {i + 1}/{alarm_iteration} times (delayed by {delay} second{'s' if delay > 0 else ''}).\")\n",
    "            winsound.PlaySound(None, winsound.SND_PURGE)\n",
    "        \n",
    "        winsound.PlaySound(done_sound, winsound.SND_FILENAME)\n",
    "        winsound.PlaySound(None, winsound.SND_PURGE)\n",
    "    else:\n",
    "        print(\"Training completed. Beep sound is not available on this OS.\")\n",
    "except KeyboardInterrupt:\n",
    "    if os.name == 'nt':\n",
    "        winsound.PlaySound(done_sound, winsound.SND_FILENAME)\n",
    "        winsound.PlaySound(None, winsound.SND_PURGE)\n",
    "\n",
    "print(\"Training completed successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
